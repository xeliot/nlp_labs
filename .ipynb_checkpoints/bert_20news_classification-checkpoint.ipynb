{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "from pytorch_transformers import *\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify 3 categories of news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories = ['talk.politics.guns', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "categories = ['soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1777 total documents between three categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "1777\n",
      "1777\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target_names) # news categories used\n",
    "print(len(twenty_train.data))\n",
    "print(len(twenty_train.filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: ds@aris.nswc.navy.mil (Demetrios Sapounas) Subject: 3D display software Organization: NSWC Lines: 19       I have the need for displaying 2 1/2 D surfaces under X, using only Xlib, Xt and Xm.  Does anyone know of a package, available on internet, which will be able to do the work?     I am looking for a STAND-ALONE package providing similar functions to \"xprism3\" available with Khoros, but without the numerous libraries required for it.  I want to be able to recompile it and run it on various platforms, from SGIs to i486s (UNIX).     Any help will be appreciated.   ======================================================================= Demetrios Sapounas                         Tel        +1 (703) 663.8332 L 115, NSWC                                Fax        +1 (703) 663.1939 Dahlgren, VA 22448-5000, USA               email  ds@aris.nswc.navy.mil ======================================================================= '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[2].replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format documents for BERT\n",
    "Add [CLS] at start and [SEP] at end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "targets = []\n",
    "#num_docs = len(twenty_train.data)\n",
    "num_docs = 100\n",
    "for i in range(num_docs):\n",
    "    docs.append(\"[CLS] \" + twenty_train.data[i].replace('\\n', ' ') + \" [SEP]\")\n",
    "    targets.append(twenty_train.target[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad / truncate inputs to be input size of 512 for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1114 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (736 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1186 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (685 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1591 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1711 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (885 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (786 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1494 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4084 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1257 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (678 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1950 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (764 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1107 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1169 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (758 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "\n",
    "max_sequence_length = 512\n",
    "# only using one sentence\n",
    "segment_ids = [1] * max_sequence_length\n",
    "for i in range(len(docs)):\n",
    "    tokenized_text = tokenizer.tokenize(docs[i])\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    if len(indexed_tokens) <= max_sequence_length:\n",
    "        padding_length = max_sequence_length - len(indexed_tokens)\n",
    "        indexed_tokens_padded = indexed_tokens + [0] * padding_length\n",
    "        input_mask = [1] * len(indexed_tokens) + [0] * padding_length\n",
    "    else:\n",
    "        indexed_tokens_padded = [indexed_tokens[0]] + indexed_tokens[1:max_sequence_length-1] + [indexed_tokens[-1]]\n",
    "        input_mask = [1] * max_sequence_length\n",
    "    inputs.append([indexed_tokens_padded, input_mask, segment_ids])\n",
    "\n",
    "inputs_tensor = torch.tensor(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 512])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network Architecture on Top of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 3)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "news_classifier = NewsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(news_classifier.parameters(), lr=0.004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop for 20 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(1.1066, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(1.1063, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(1.0790, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(1.0720, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(1.0284, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.8993, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.8857, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.7891, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.7438, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.5641, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.5295, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.4306, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.3056, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.2870, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.2342, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.2577, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.1698, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.1646, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.1321, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.1251, grad_fn=<NllLossBackward>)\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    example_tokens_tensor = inputs_tensor[:, 0, :]\n",
    "    example_input_mask = inputs_tensor[:, 1, :]\n",
    "    example_segment_ids = inputs_tensor[:, 2, :]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = bert_model(example_tokens_tensor, token_type_ids=example_segment_ids, attention_mask=example_input_mask)\n",
    "    \n",
    "    # 0th element because we only care about [CLS] token\n",
    "    classifier_inputs = encoded_layers[:, 0, :]\n",
    "    \n",
    "    \n",
    "    outputs = news_classifier(classifier_inputs)\n",
    "    targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "    \n",
    "    loss = criterion(outputs, targets_tensor)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"Loss\", loss)\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to predict new news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_news_article(sentence, bert_model):\n",
    "    marked = \"[CLS] \" + sentence.replace('\\n', ' ') + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    max_sequence_length = 512\n",
    "    # only using one sentence\n",
    "    segment_ids = [1] * max_sequence_length\n",
    "    if len(indexed_tokens) <= max_sequence_length:\n",
    "        padding_length = max_sequence_length - len(indexed_tokens)\n",
    "        indexed_tokens_padded = indexed_tokens + [0] * padding_length\n",
    "        input_mask = [1] * len(indexed_tokens) + [0] * padding_length\n",
    "    else:\n",
    "        indexed_tokens_padded = [indexed_tokens[0]] + indexed_tokens[1:max_sequence_length-1] + [indexed_tokens[-1]]\n",
    "        input_mask = [1] * max_sequence_length\n",
    "    \n",
    "    inputs = [[indexed_tokens_padded, input_mask, segment_ids]]\n",
    "    inputs_tensor = torch.tensor(inputs)\n",
    "    \n",
    "    indexed_tokens_padded = inputs_tensor[:, 0, :]\n",
    "    segment_ids = inputs_tensor[:, 1, :]\n",
    "    input_mask = inputs_tensor[:, 2, :]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = bert_model(indexed_tokens_padded, token_type_ids=segment_ids, attention_mask=input_mask)\n",
    "        \n",
    "    classifier_inputs = encoded_layers[:, 0, :]\n",
    "    \n",
    "    \n",
    "    outputs = news_classifier(classifier_inputs)\n",
    "    return twenty_train.target_names[torch.argmax(outputs).item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Religious Article Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We started with Genesis 1 and God's intentions for his world. The story concluded, 'Then God said, 'Let us make people in our image. He made a man out of the dust of the earth and God breathed his spirit into the man. So Adam became a living being. Later God put Adam to sleep and took one of his ribs and made a wife, Eve, for him. God said, 'Rule over the animals … multiply and fill the earth.' Finally God looked at everything he had made and blessed it. He said, 'It is very good.' On the seventh day God rested from his work because he had completed the work of creation.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soc.religion.christian'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_news_article(\"We started with Genesis 1 and God's intentions for his world. The story concluded, 'Then God said, 'Let us make people in our image. He made a man out of the dust of the earth and God breathed his spirit into the man. So Adam became a living being. Later God put Adam to sleep and took one of his ribs and made a wife, Eve, for him. God said, 'Rule over the animals … multiply and fill the earth.' Finally God looked at everything he had made and blessed it. He said, 'It is very good.' On the seventh day God rested from his work because he had completed the work of creation.'\", bert_model=bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Science Article Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regulators in the United States have already approved deep brain stimulation for the treatment of Parkinson's disease, epilepsy, essential tremor, and obsessive-compulsive disorder. The treatment involves implanting wires into the brain and a stimulator in the chest or abdomen. The stimulator sends small electrical pulses to the wires along a connection lead under the skin. Doctors sometimes refer to the stimulator as a pacemaker. The surgeons implant the wires into areas of the brain that are responsible for the symptoms of the particular condition. In the case of Parkinson's disease, for example, they implant them into the brain area that controls movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sci.med'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_news_article(\"Regulators in the United States have already approved deep brain stimulation for the treatment of Parkinson's disease, epilepsy, essential tremor, and obsessive-compulsive disorder. The treatment involves implanting wires into the brain and a stimulator in the chest or abdomen. The stimulator sends small electrical pulses to the wires along a connection lead under the skin. Doctors sometimes refer to the stimulator as a pacemaker. The surgeons implant the wires into areas of the brain that are responsible for the symptoms of the particular condition. In the case of Parkinson's disease, for example, they implant them into the brain area that controls movement.\", bert_model=bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computer Graphics Article Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper presents a surface reconstruction algorithm that takes an unoriented point cloud as input and produces an interpolating surface in the form of triangulation. Based on region-growing and Delaunay approaches, this algorithm aims to address the difficulties of reconstruction from point data with imperfections. Starting with a seed triangle from the Delaunay tetrahedron result for input points, the surface is gradually formed by adding the linked Delaunay triangle from the current boundaries one by one. During surface growth, the topology errors and the quantity of the holes generated by adding inappropriate triangles can be reduced by changing the triangle selection criteria and adjusting the addition order of the triangles. We evaluated our method using a wide range of datasets, and this method compares well to popular classic and current algorithms with unoriented input points and triangulated surface output. In addition, to achieve results with a small number of holes on the generated surface, a detection and repair approach is proposed to turn the holes of various shapes into smooth surfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.graphics'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_news_article(\"This paper presents a surface reconstruction algorithm that takes an unoriented point cloud as input and produces an interpolating surface in the form of triangulation. Based on region-growing and Delaunay approaches, this algorithm aims to address the difficulties of reconstruction from point data with imperfections. Starting with a seed triangle from the Delaunay tetrahedron result for input points, the surface is gradually formed by adding the linked Delaunay triangle from the current boundaries one by one. During surface growth, the topology errors and the quantity of the holes generated by adding inappropriate triangles can be reduced by changing the triangle selection criteria and adjusting the addition order of the triangles. We evaluated our method using a wide range of datasets, and this method compares well to popular classic and current algorithms with unoriented input points and triangulated surface output. In addition, to achieve results with a small number of holes on the generated surface, a detection and repair approach is proposed to turn the holes of various shapes into smooth surfaces.\", bert_model=bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
