{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import os\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Pre-trained word vectors Glove zip\n",
    "http://nlp.stanford.edu/data/glove.6B.zip  \n",
    "Unzip 6B zip file from Downloads  \n",
    "Copy \"glove.6B.300d.txt\" from unzipped folder  \n",
    "Create folder named \"data\" and paste copied text file into \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pre-trained word vectors from Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "glove_file = datapath(os.getcwd()+\"/data/glove.6B.300d.txt\")\n",
    "word2vec_glove_file = get_tmpfile(os.getcwd()+\"/data/glove.6B.300d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch 20 News Groups Data\n",
    "4 categories\n",
    " - gun politics\n",
    " - christianity\n",
    " - computer graphics\n",
    " - science medical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.politics.guns', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.graphics', 'sci.med', 'soc.religion.christian', 'talk.politics.guns']\n",
      "2323\n",
      "2323\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target_names) # news categories used\n",
    "print(len(twenty_train.data))\n",
    "print(len(twenty_train.filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add spaces before and after punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From :  Petch @ gvg47 . gvg . tek . com  ( Chuck Petch ) \n",
      "Subject :  Daily Verse\n",
      "Organization :  Grass Valley Group ,  Grass Valley ,  CA\n",
      "Lines :  4\n",
      "\n",
      "For whoever does the will of my Father in heaven is my brother and sister\n",
      "and mother .  \"  \n",
      "\n",
      "Matthew 12 : 50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
    "print(\"\\n\".join(clean_text(twenty_train.data[2]).split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of preprocessing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', ':', 'petch', '@', 'gvg47', '.', 'gvg', '.', 'tek', '.', 'com', '(', 'chuck', 'petch', ')', 'subject', ':', 'daily', 'verse', 'organization', ':', 'grass', 'valley', 'group', ',', 'grass', 'valley', ',', 'ca', 'lines', ':', '4', 'for', 'whoever', 'does', 'the', 'will', 'of', 'my', 'father', 'in', 'heaven', 'is', 'my', 'brother', 'and', 'sister', 'and', 'mother', '.', '\"', 'matthew', '12', ':', '50']\n"
     ]
    }
   ],
   "source": [
    "print(clean_text(twenty_train.data[2]).replace('\\n', ' ').lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize sequence length distribution to determine appropriate bin separations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAES9JREFUeJzt3X+s3XV9x/HnS0BwulkqF1LbJkXtpmhiIXdYx/5g4JQfy4qJLJBFGtakbsEMF7OtuGToMhJMVDYSR6wDqcaBDHE0yHSsYox/CN5qrZTCuEpHr+3odfwQZ8YsvvfH+Vx7rLfc03vO6b3tfT6Sb873+/l+vt/z+X76vXn1+/OkqpAkLWwvmesGSJLmnmEgSTIMJEmGgSQJw0CShGEgScIwkCRhGEiSMAwkScDxc90AgFNOOaVWrFgx182QpKPK1q1bf1hVI4NY17wIgxUrVjA2NjbXzZCko0qS/xzUujxNJEkyDCRJhoEkCcNAkoRhIEnCMJAk0UMYJDkpyYNJvpNkR5IPtfJbkzyeZFsbVrXyJLkxyXiS7UnOGvZGSJL608tzBs8D51XVj5OcAHw9yb+2eX9eVXceVP9CYGUb3gLc1D4lSfPUjEcG1fHjNnlCG17sh5PXAJ9uy30DWJRkSf9NlSQNS09PICc5DtgKvA74eFU9kORPgOuS/DWwBdhQVc8DS4HdXYtPtLK9A235UWbFhi8O/Tt2XX/x0L9D0rGppwvIVfVCVa0ClgFnJ3kTcA3weuA3gcXAX7bqmW4VBxckWZ9kLMnY5OTkrBovSRqMw7qbqKqeAb4KXFBVe9upoOeBTwFnt2oTwPKuxZYBe6ZZ18aqGq2q0ZGRgbxnSZI0S73cTTSSZFEbfxnwNuCRqesASQJcAjzUFtkMXNHuKloNPFtVC/oUkSTNd71cM1gCbGrXDV4C3FFV9yT5SpIROqeFtgF/3OrfC1wEjAM/Aa4cfLMlSYM0YxhU1XbgzGnKzztE/QKu6r9pkqQjxSeQJUmGgSTJMJAkYRhIkjAMJEkYBpIkDANJEj2+qG4hOBIvkpOk+cojA0mSYSBJMgwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEn4bqJjyrDfr7Tr+ouHun5Jc8cjA0nSzGGQ5KQkDyb5TpIdST7Uyk9P8kCSx5J8LslLW/mJbXq8zV8x3E2QJPWrlyOD54HzqurNwCrggiSrgQ8DN1TVSuBpYF2rvw54uqpeB9zQ6kmS5rEZw6A6ftwmT2hDAecBd7byTcAlbXxNm6bNPz9JBtZiSdLA9XTNIMlxSbYB+4D7gO8Bz1TV/lZlAljaxpcCuwHa/GeBV02zzvVJxpKMTU5O9rcVkqS+9BQGVfVCVa0ClgFnA2+Yrlr7nO4ooH6poGpjVY1W1ejIyEiv7ZUkDcFh3U1UVc8AXwVWA4uSTN2augzY08YngOUAbf4rgacG0VhJ0nD0cjfRSJJFbfxlwNuAncD9wLtatbXA3W18c5umzf9KVf3SkYEkaf7o5aGzJcCmJMfRCY87quqeJA8Dtyf5W+DbwM2t/s3AZ5KM0zkiuGwI7ZYkDdCMYVBV24Ezpyn/Pp3rBweX/y9w6UBaJ0k6InwCWZJkGEiSDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CSRG+vsJYAWLHhi0Nd/67rLx7q+iUdmkcGkiTDQJJkGEiSMAwkSRgGkiQMA0kSPYRBkuVJ7k+yM8mOJFe38g8m+UGSbW24qGuZa5KMJ3k0yTuGuQGSpP718pzBfuD9VfWtJL8KbE1yX5t3Q1V9pLtykjOAy4A3Aq8G/j3Jr1fVC4NsuCRpcGY8MqiqvVX1rTb+HLATWPoii6wBbq+q56vqcWAcOHsQjZUkDcdhXTNIsgI4E3igFb03yfYktyQ5uZUtBXZ3LTbBi4eHJGmO9RwGSV4BfB54X1X9CLgJeC2wCtgLfHSq6jSL1zTrW59kLMnY5OTkYTdckjQ4PYVBkhPoBMFnq+ougKp6sqpeqKqfAZ/kwKmgCWB51+LLgD0Hr7OqNlbVaFWNjoyM9LMNkqQ+9XI3UYCbgZ1V9bGu8iVd1d4JPNTGNwOXJTkxyenASuDBwTVZkjRovdxNdA7wbuC7Sba1sg8AlydZRecU0C7gPQBVtSPJHcDDdO5Euso7iSRpfpsxDKrq60x/HeDeF1nmOuC6PtolSTqCfAJZkmQYSJIMA0kS/uyl5pFh/6wm+NOa0qF4ZCBJMgwkSYaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSfQQBkmWJ7k/yc4kO5Jc3coXJ7kvyWPt8+RWniQ3JhlPsj3JWcPeCElSf3o5MtgPvL+q3gCsBq5KcgawAdhSVSuBLW0a4EJgZRvWAzcNvNWSpIGaMQyqam9VfauNPwfsBJYCa4BNrdom4JI2vgb4dHV8A1iUZMnAWy5JGpjDumaQZAVwJvAAcFpV7YVOYACntmpLgd1di020MknSPNVzGCR5BfB54H1V9aMXqzpNWU2zvvVJxpKMTU5O9toMSdIQ9BQGSU6gEwSfraq7WvGTU6d/2ue+Vj4BLO9afBmw5+B1VtXGqhqtqtGRkZHZtl+SNAC93E0U4GZgZ1V9rGvWZmBtG18L3N1VfkW7q2g18OzU6SRJ0vx0fA91zgHeDXw3ybZW9gHgeuCOJOuAJ4BL27x7gYuAceAnwJUDbbEkaeBmDIOq+jrTXwcAOH+a+gVc1We7JElHkE8gS5IMA0mSYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkughDJLckmRfkoe6yj6Y5AdJtrXhoq551yQZT/JokncMq+GSpMHp5cjgVuCCacpvqKpVbbgXIMkZwGXAG9sy/5DkuEE1VpI0HDOGQVV9DXiqx/WtAW6vquer6nFgHDi7j/ZJko6Afq4ZvDfJ9nYa6eRWthTY3VVnopVJkuax2YbBTcBrgVXAXuCjrTzT1K3pVpBkfZKxJGOTk5OzbIYkaRBmFQZV9WRVvVBVPwM+yYFTQRPA8q6qy4A9h1jHxqoararRkZGR2TRDkjQgswqDJEu6Jt8JTN1ptBm4LMmJSU4HVgIP9tdESdKwHT9ThSS3AecCpySZAK4Fzk2yis4poF3AewCqakeSO4CHgf3AVVX1wnCaLkkalBnDoKoun6b45hepfx1wXT+NkiQdWT6BLEkyDCRJhoEkCcNAkkQPF5ClY8mKDV8c6vp3XX/xUNcvDctREwbD/iOWpIXM00SSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgSeIoegJZOhr4ugsdrTwykCQZBpIkw0CShGEgSaKHMEhyS5J9SR7qKluc5L4kj7XPk1t5ktyYZDzJ9iRnDbPxkqTB6OXI4FbggoPKNgBbqmolsKVNA1wIrGzDeuCmwTRTkjRMM4ZBVX0NeOqg4jXApja+Cbikq/zT1fENYFGSJYNqrCRpOGZ7zeC0qtoL0D5PbeVLgd1d9SZamSRpHhv0BeRMU1bTVkzWJxlLMjY5OTngZkiSDsdsw+DJqdM/7XNfK58AlnfVWwbsmW4FVbWxqkaranRkZGSWzZAkDcJsw2AzsLaNrwXu7iq/ot1VtBp4dup0kiRp/prx3URJbgPOBU5JMgFcC1wP3JFkHfAEcGmrfi9wETAO/AS4cghtliQN2IxhUFWXH2LW+dPULeCqfhslSTqyfAJZkmQYSJIMA0kS/riNdFTxx3M0LB4ZSJIMA0mSYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSfgKa0ldhv2KbPA12fOVRwaSJMNAktTnaaIku4DngBeA/VU1mmQx8DlgBbAL+IOqerq/ZkqShmkQRwa/U1Wrqmq0TW8AtlTVSmBLm5YkzWPDOE20BtjUxjcBlwzhOyRJA9RvGBTwb0m2Jlnfyk6rqr0A7fPU6RZMsj7JWJKxycnJPpshSepHv7eWnlNVe5KcCtyX5JFeF6yqjcBGgNHR0eqzHZKkPvR1ZFBVe9rnPuALwNnAk0mWALTPff02UpI0XLM+MkjycuAlVfVcG3878DfAZmAtcH37vHsQDZV0bBj2g20+1DY7/ZwmOg34QpKp9fxTVX0pyTeBO5KsA54ALu2/mZKkYZp1GFTV94E3T1P+38D5/TRKknRk+QSyJMkwkCQZBpIkfIW1JM0rR+I14tPxyECSZBhIkjxNJOkYM1enWY52HhlIkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIYYhgkuSDJo0nGk2wY1vdIkvo3lDBIchzwceBC4Azg8iRnDOO7JEn9G9aRwdnAeFV9v6r+D7gdWDOk75Ik9WlYYbAU2N01PdHKJEnz0LB+6SzTlNUvVEjWA+vb5PNJHhpSW442pwA/nOtGzBP2xQH2xQH2xQG/MagVDSsMJoDlXdPLgD3dFapqI7ARIMlYVY0OqS1HFfviAPviAPviAPvigCRjg1rXsE4TfRNYmeT0JC8FLgM2D+m7JEl9GsqRQVXtT/Je4MvAccAtVbVjGN8lSerfsE4TUVX3Avf2WH3jsNpxFLIvDrAvDrAvDrAvDhhYX6SqZq4lSTqm+ToKSdLch8FCem1FkuVJ7k+yM8mOJFe38sVJ7kvyWPs8uZUnyY2tb7YnOWtut2DwkhyX5NtJ7mnTpyd5oPXF59oNCCQ5sU2Pt/kr5rLdg5ZkUZI7kzzS9o+3LtT9Ismftb+Ph5LcluSkhbRfJLklyb7u2+1nsy8kWdvqP5Zk7UzfO6dhsABfW7EfeH9VvQFYDVzVtncDsKWqVgJb2jR0+mVlG9YDNx35Jg/d1cDOrukPAze0vngaWNfK1wFPV9XrgBtavWPJ3wNfqqrXA2+m0ycLbr9IshT4U2C0qt5E5waUy1hY+8WtwAUHlR3WvpBkMXAt8BY6b4S4dipADqmq5mwA3gp8uWv6GuCauWzTEd7+u4HfBR4FlrSyJcCjbfwTwOVd9X9e71gY6Dx/sgU4D7iHzsOKPwSOP3j/oHNn2lvb+PGtXuZ6GwbUD78GPH7w9izE/YIDby9Y3P6d7wHesdD2C2AF8NBs9wXgcuATXeW/UG+6Ya5PEy3Y11a0w9kzgQeA06pqL0D7PLVVO9b75++AvwB+1qZfBTxTVfvbdPf2/rwv2vxnW/1jwWuASeBT7ZTZPyZ5OQtwv6iqHwAfAZ4A9tL5d97Kwtwvuh3uvnDY+8hch8GMr604FiV5BfB54H1V9aMXqzpN2THRP0l+D9hXVVu7i6epWj3MO9odD5wF3FRVZwL/w4HTANM5ZvuincpYA5wOvBp4OZ1TIQdbCPtFLw61/YfdL3MdBjO+tuJYk+QEOkHw2aq6qxU/mWRJm78E2NfKj+X+OQf4/SS76LzV9jw6RwqLkkw9/9K9vT/vizb/lcBTR7LBQzQBTFTVA236TjrhsBD3i7cBj1fVZFX9FLgL+C0W5n7R7XD3hcPeR+Y6DBbUayuSBLgZ2FlVH+uatRmYutq/ls61hKnyK9odA6uBZ6cOFY92VXVNVS2rqhV0/t2/UlV/CNwPvKtVO7gvpvroXa3+MfE/wKr6L2B3kqmXjp0PPMwC3C/onB5aneRX2t/LVF8suP3iIIe7L3wZeHuSk9vR1ttb2aHNgwslFwH/AXwP+Ku5bs+Qt/W36RyqbQe2teEiOuc4twCPtc/FrX7o3G31PeC7dO6wmPPtGEK/nAvc08ZfAzwIjAP/DJzYyk9q0+Nt/mvmut0D7oNVwFjbN/4FOHmh7hfAh4BHgIeAzwAnLqT9AriNzvWSn9L5H/662ewLwB+1fhkHrpzpe30CWZI056eJJEnzgGEgSTIMJEmGgSQJw0CShGEgScIwkCRhGEiSgP8HM8dWb7ktW/4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_word_length_dist = []\n",
    "for article in twenty_train.data:\n",
    "    cleaned_article = clean_text(article).replace('\\n', ' ').lower().split()\n",
    "    pos_word_length_dist.append(len(cleaned_article))\n",
    "\n",
    "plt.hist(pos_word_length_dist, bins=400)\n",
    "plt.xlim(0, 1000)\n",
    "#sns.distplot(pos_word_length_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data into bins to feed into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(data, target):\n",
    "    random_seed = np.random.randint(low=len(data), size=len(data))\n",
    "    shuffled_data = data[random_seed]\n",
    "    shuffled_target = target[random_seed]\n",
    "    return (shuffled_data, shuffled_target)\n",
    "\n",
    "def fit_sequence(sequence, size):\n",
    "    #print(type(sequence))\n",
    "    #print(sequence)\n",
    "    if len(sequence) == size:\n",
    "        return sequence\n",
    "    elif len(sequence) > size:\n",
    "        return sequence[:size]\n",
    "    elif len(sequence) < size:\n",
    "        #print(size-len(sequence))\n",
    "        return np.concatenate((sequence, np.zeros((int(size-len(sequence)), 300), dtype=float)))\n",
    "\n",
    "def organize_into_bins(documents, target):\n",
    "    np.array([np.array([]), np.array([]), np.array([]), np.array([])])\n",
    "    bin_input = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "    bin_target = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "    batch_lengths = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "    \n",
    "    for i in range(0, len(documents)):\n",
    "        if len(documents[i]) <= 1000:\n",
    "            index = len(documents[i]) // 100\n",
    "            batch_lengths[index].append(len(documents[i]))\n",
    "        else:\n",
    "            batch_lengths[10].append(len(documents[i]))\n",
    "            \n",
    "    for i in range(0, len(documents)):\n",
    "        if len(documents[i]) <= 1000:\n",
    "            index = len(documents[i]) // 100\n",
    "            cutoff = math.floor(np.percentile(np.array(batch_lengths[index]), q=80))\n",
    "            bin_input[index].append(fit_sequence(documents[i], cutoff))\n",
    "            bin_target[index].append(target[i])\n",
    "        else:\n",
    "            bin_input[10].append(fit_sequence(documents[i], 1000))\n",
    "            bin_target[10].append(target[i])\n",
    "    return(bin_input, bin_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess raw data and place into appropriate bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "target = []\n",
    "for i in range(0, len(twenty_train.data)):\n",
    "    article_vector = []\n",
    "    cleaned_article = clean_text(twenty_train.data[i]).replace('\\n', ' ').lower().split()\n",
    "    for word in cleaned_article:\n",
    "        try:\n",
    "            article_vector.append(model[word])\n",
    "        except KeyError:\n",
    "            article_vector.append(np.zeros(300, dtype=float))\n",
    "    data.append(article_vector)\n",
    "    target.append(twenty_train.target[i])\n",
    "\n",
    "(data, target) = shuffle(data=np.array(data), target=np.array(target))\n",
    "input_test = data[:300]\n",
    "target_test = target[:300]\n",
    "input_validation = data[300:600]\n",
    "target_validation = target[300:600]\n",
    "input_train = data[600:]\n",
    "target_train = target[600:]\n",
    "(bin_input, bin_target) = organize_into_bins(documents=input_train, target=target_train)\n",
    "(bin_input_validation, bin_target_validation) = organize_into_bins(documents=input_validation, target=target_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205, 469, 300)\n",
      "(205,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(bin_input[4]).shape)\n",
    "print(np.array(bin_target[4]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM model with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, layer_dim=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm1 = nn.LSTM(input_size=300, hidden_size=hidden_size, num_layers=layer_dim, batch_first=True, bidirectional=True, dropout=0.4)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, 20) # hidden_size *2 because of bidirectional\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.fc3 = nn.Linear(20, 4)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out, (hn, cn) = self.lstm1(inputs)\n",
    "        hn_flat = torch.cat((hn[0], hn[1]), dim=1)\n",
    "        out = F.relu(self.fc1(hn_flat))\n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "net = LSTM(hidden_size=20, layer_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Loss: 1.408562497659163 |  Validation Loss: 1.4109127846631138\n",
      "Running Loss: 1.3812840960242532 |  Validation Loss: 1.3840269392186946\n",
      "Running Loss: 1.3756905794143677 |  Validation Loss: 1.3804090456529097\n",
      "Running Loss: 1.3703175566413186 |  Validation Loss: 1.3773631074211814\n",
      "Running Loss: 1.353592038154602 |  Validation Loss: 1.3766004172238437\n",
      "Running Loss: 1.3342285589738325 |  Validation Loss: 1.3545850081877275\n",
      "Running Loss: 1.2805939804423938 |  Validation Loss: 1.3369362245906482\n",
      "Running Loss: 1.1803498430685564 |  Validation Loss: 1.2436537634242664\n",
      "Running Loss: 1.0525894923643633 |  Validation Loss: 1.0497095422311262\n",
      "Running Loss: 0.8780980950052087 |  Validation Loss: 0.8599239208481528\n",
      "Running Loss: 0.7509602958505804 |  Validation Loss: 0.7490809654647653\n",
      "Running Loss: 0.6154901358214292 |  Validation Loss: 0.6953254802660509\n",
      "Running Loss: 0.5575180920687589 |  Validation Loss: 0.6215756156227805\n",
      "Running Loss: 0.448460075665604 |  Validation Loss: 0.5384148941798643\n",
      "Running Loss: 0.41349567202004517 |  Validation Loss: 0.6276193057948892\n",
      "Running Loss: 0.3143323842774738 |  Validation Loss: 0.5119629273699089\n",
      "Running Loss: 0.28306732191280887 |  Validation Loss: 0.5133374020541933\n",
      "Running Loss: 0.282681943340735 |  Validation Loss: 0.4170313537205485\n",
      "Running Loss: 0.23077572069384836 |  Validation Loss: 0.5218243301338092\n",
      "Running Loss: 0.1596479734236544 |  Validation Loss: 0.3315709298264913\n",
      "Running Loss: 0.19256293739784847 |  Validation Loss: 0.6467244875490327\n",
      "Running Loss: 0.14424354371360756 |  Validation Loss: 0.21495652637994764\n",
      "Running Loss: 0.13481479646130043 |  Validation Loss: 0.4307843995735642\n",
      "Running Loss: 0.1094706320965832 |  Validation Loss: 0.4182920687876917\n",
      "Running Loss: 0.09304701536893845 |  Validation Loss: 0.3666562213898446\n",
      "Running Loss: 0.09627889401533386 |  Validation Loss: 0.36952134479162685\n",
      "Running Loss: 0.08814703156663613 |  Validation Loss: 0.4502568897875211\n",
      "Running Loss: 0.0701036544686014 |  Validation Loss: 0.38259878786580387\n",
      "Running Loss: 0.07024344869635323 |  Validation Loss: 0.23468655212358994\n",
      "Running Loss: 0.06053782559253953 |  Validation Loss: 0.2289265693154696\n",
      "Running Loss: 0.07019755136306313 |  Validation Loss: 0.23482757120245878\n",
      "Running Loss: 0.05042874355885116 |  Validation Loss: 0.24942390719872326\n",
      "Running Loss: 0.04499604066156528 |  Validation Loss: 0.2170307977172673\n",
      "Running Loss: 0.047037881312214515 |  Validation Loss: 0.2157806575758679\n",
      "Running Loss: 0.04988943611864339 |  Validation Loss: 0.21709513752958545\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(35):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    running_validation_loss = 0.0\n",
    "    for i in range(0, len(bin_input)):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        net.train()\n",
    "        \n",
    "        batch_input = np.array(bin_input[i])\n",
    "        batch_target = np.array(bin_target[i])\n",
    "        batch_input = torch.from_numpy(batch_input)\n",
    "        batch_input = batch_input.type(torch.float)\n",
    "        batch_target = torch.from_numpy(batch_target)\n",
    "        batch_target = batch_target.type(torch.long)\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(batch_input)\n",
    "        #print(outputs.shape)\n",
    "        loss = criterion(outputs, batch_target)\n",
    "        \n",
    "        net.eval()\n",
    "        batch_input_validation = np.array(bin_input_validation[i])\n",
    "        batch_target_validation = np.array(bin_target_validation[i])\n",
    "        batch_input_validation = torch.from_numpy(batch_input_validation)\n",
    "        batch_input_validation = batch_input_validation.type(torch.float)\n",
    "        batch_target_validation = torch.from_numpy(batch_target_validation)\n",
    "        batch_target_validation = batch_target_validation.type(torch.long)\n",
    "        \n",
    "        validation_outputs = net(batch_input_validation)\n",
    "        validation_loss = criterion(validation_outputs, batch_target_validation)\n",
    "        net.train()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_validation_loss += validation_loss.item()\n",
    "    print(\"Running Loss: \" + str((running_loss / len(bin_input))) + \" |  Validation Loss: \" + str((running_validation_loss / len(bin_input))))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "\n",
    "def predict(review):\n",
    "    parse_review = clean_text(review).replace('\\n', ' ').lower().split()\n",
    "    parse_review_vectors = []\n",
    "    for word in parse_review:\n",
    "        try:\n",
    "            parse_review_vectors.append(model[word])\n",
    "        except KeyError:\n",
    "            parse_review_vectors.append(np.zeros(300, dtype=float))\n",
    "    review_tensor = torch.tensor(parse_review_vectors, dtype=torch.float)\n",
    "    review_tensor = review_tensor.view(1, len(parse_review_vectors), 300)\n",
    "    return (net(review_tensor) > 0.5).item()\n",
    "\n",
    "def predict_from_vector(review_vector):\n",
    "    review_tensor = torch.tensor(review_vector, dtype=torch.float)\n",
    "    review_tensor = review_tensor.view(1, len(review_vector), 300)\n",
    "    prediction = net(review_tensor)\n",
    "    return torch.argmax(prediction.view(prediction.size(1)))\n",
    "\n",
    "def class_report(input_test, ground_truth):\n",
    "    prediction = []\n",
    "    for document_vector in input_test:\n",
    "        prediction.append(predict_from_vector(document_vector))\n",
    "    prediction = np.array(prediction)\n",
    "    ground_truth = np.array(ground_truth)\n",
    "    print(confusion_matrix(ground_truth, prediction))  \n",
    "    print(classification_report(ground_truth, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70  2  1  7]\n",
      " [ 0 67  2  4]\n",
      " [ 1  1 59  1]\n",
      " [ 0  1  3 81]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93        80\n",
      "           1       0.94      0.92      0.93        73\n",
      "           2       0.91      0.95      0.93        62\n",
      "           3       0.87      0.95      0.91        85\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       300\n",
      "   macro avg       0.93      0.92      0.92       300\n",
      "weighted avg       0.93      0.92      0.92       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_report(input_test=input_test, ground_truth=target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
