{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "from pytorch_transformers import *\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify 3 categories of news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories = ['talk.politics.guns', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "categories = ['soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1777 total documents between three categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "1777\n",
      "1777\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target_names) # news categories used\n",
    "print(len(twenty_train.data))\n",
    "print(len(twenty_train.filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: ds@aris.nswc.navy.mil (Demetrios Sapounas) Subject: 3D display software Organization: NSWC Lines: 19       I have the need for displaying 2 1/2 D surfaces under X, using only Xlib, Xt and Xm.  Does anyone know of a package, available on internet, which will be able to do the work?     I am looking for a STAND-ALONE package providing similar functions to \"xprism3\" available with Khoros, but without the numerous libraries required for it.  I want to be able to recompile it and run it on various platforms, from SGIs to i486s (UNIX).     Any help will be appreciated.   ======================================================================= Demetrios Sapounas                         Tel        +1 (703) 663.8332 L 115, NSWC                                Fax        +1 (703) 663.1939 Dahlgren, VA 22448-5000, USA               email  ds@aris.nswc.navy.mil ======================================================================= '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[2].replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format documents for BERT\n",
    "Add [CLS] at start and [SEP] at end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "targets = []\n",
    "#num_docs = len(twenty_train.data)\n",
    "num_docs = 100\n",
    "for i in range(num_docs):\n",
    "    docs.append(\"[CLS] \" + twenty_train.data[i].replace('\\n', ' ') + \" [SEP]\")\n",
    "    targets.append(twenty_train.target[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad / truncate inputs to be input size of 512 for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1114 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (736 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1186 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (685 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1591 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1711 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (885 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (786 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1494 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4084 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1257 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (678 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1950 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (764 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1107 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1169 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (758 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "\n",
    "max_sequence_length = 512\n",
    "# Each training record uses only one sentence\n",
    "segment_ids = [1] * max_sequence_length\n",
    "for i in range(len(docs)):\n",
    "    tokenized_text = tokenizer.tokenize(docs[i])\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    indexed_tokens_padded = indexed_tokens\n",
    "    input_mask = [1] * len(indexed_tokens)\n",
    "    \n",
    "    if len(indexed_tokens) <= max_sequence_length:\n",
    "        # Pad the sequence to 512\n",
    "        padding_length = max_sequence_length - len(indexed_tokens)\n",
    "        indexed_tokens_padded = indexed_tokens + [0] * padding_length\n",
    "        input_mask = [1] * len(indexed_tokens) + [0] * padding_length\n",
    "    else:\n",
    "        # Truncate the sequence to 512\n",
    "        indexed_tokens_padded = [indexed_tokens[0]] + indexed_tokens[1:max_sequence_length-1] + [indexed_tokens[-1]]\n",
    "        input_mask = [1] * max_sequence_length\n",
    "    inputs.append([indexed_tokens_padded, input_mask, segment_ids])\n",
    "\n",
    "inputs_tensor = torch.tensor(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network Architecture on Top of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        # 768 is the feature size of Bert output\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        # 3 is the number of output classes\n",
    "        self.fc4 = nn.Linear(256, 3)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewsClassifier(\n",
       "  (fc1): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc4): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "news_classifier = NewsClassifier()\n",
    "news_classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(news_classifier.parameters(), lr=0.004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop for 20 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(1.0960, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(1.1197, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(1.1278, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(1.0645, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(1.0059, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.9570, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.8010, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.6973, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.6496, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.5651, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.3903, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.4106, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.3041, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.3404, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.2550, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.1559, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.2084, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.1625, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.1148, grad_fn=<NllLossBackward>)\n",
      "Loss tensor(0.1919, grad_fn=<NllLossBackward>)\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    example_tokens_tensor = inputs_tensor[:, 0, :]\n",
    "    example_input_mask = inputs_tensor[:, 1, :]\n",
    "    example_segment_ids = inputs_tensor[:, 2, :]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = bert_model(example_tokens_tensor, token_type_ids=example_segment_ids, attention_mask=example_input_mask)\n",
    "    \n",
    "    # 0th element because we only care about [CLS] token\n",
    "    classifier_inputs = encoded_layers[:, 0, :]\n",
    "    \n",
    "    \n",
    "    outputs = news_classifier(classifier_inputs)\n",
    "    targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "    \n",
    "    loss = criterion(outputs, targets_tensor)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"Loss\", loss)\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to predict category of news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_news_article(sentence, bert_model, classifier):\n",
    "    marked = \"[CLS] \" + sentence.replace('\\n', ' ') + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    max_sequence_length = 512\n",
    "    # only using one sentence\n",
    "    segment_ids = [1] * max_sequence_length\n",
    "    if len(indexed_tokens) <= max_sequence_length:\n",
    "        padding_length = max_sequence_length - len(indexed_tokens)\n",
    "        indexed_tokens_padded = indexed_tokens + [0] * padding_length\n",
    "        input_mask = [1] * len(indexed_tokens) + [0] * padding_length\n",
    "    else:\n",
    "        indexed_tokens_padded = [indexed_tokens[0]] + indexed_tokens[1:max_sequence_length-1] + [indexed_tokens[-1]]\n",
    "        input_mask = [1] * max_sequence_length\n",
    "    \n",
    "    inputs = [[indexed_tokens_padded, input_mask, segment_ids]]\n",
    "    inputs_tensor = torch.tensor(inputs)\n",
    "    \n",
    "    indexed_tokens_padded = inputs_tensor[:, 0, :]\n",
    "    segment_ids = inputs_tensor[:, 2, :]\n",
    "    input_mask = inputs_tensor[:, 1, :]\n",
    "    \n",
    "    bert_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = bert_model(indexed_tokens_padded, token_type_ids=segment_ids, attention_mask=input_mask)\n",
    "        \n",
    "    classifier.eval()\n",
    "    classifier_inputs = encoded_layers[:, 0, :]\n",
    "    \n",
    "    \n",
    "    outputs = classifier(classifier_inputs)\n",
    "    return twenty_train.target_names[torch.argmax(outputs).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2618 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 300 Truth = soc.religion.christian Predicted = soc.religion.christian\n",
      "item 301 Truth = soc.religion.christian Predicted = soc.religion.christian\n",
      "item 302 Truth = sci.med Predicted = sci.med\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1389 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 303 Truth = comp.graphics Predicted = comp.graphics\n",
      "item 304 Truth = soc.religion.christian Predicted = soc.religion.christian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (909 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 305 Truth = sci.med Predicted = sci.med\n",
      "item 306 Truth = soc.religion.christian Predicted = soc.religion.christian\n",
      "item 307 Truth = soc.religion.christian Predicted = soc.religion.christian\n",
      "item 308 Truth = comp.graphics Predicted = comp.graphics\n",
      "item 309 Truth = comp.graphics Predicted = comp.graphics\n",
      "item 310 Truth = comp.graphics Predicted = comp.graphics\n",
      "item 311 Truth = sci.med Predicted = sci.med\n",
      "item 312 Truth = comp.graphics Predicted = comp.graphics\n",
      "item 313 Truth = soc.religion.christian Predicted = soc.religion.christian\n",
      "item 314 Truth = comp.graphics Predicted = comp.graphics\n",
      "item 315 Truth = sci.med Predicted = sci.med\n",
      "item 316 Truth = sci.med Predicted = sci.med\n",
      "item 317 Truth = comp.graphics Predicted = comp.graphics\n",
      "item 318 Truth = comp.graphics Predicted = comp.graphics\n",
      "item 319 Truth = sci.med Predicted = comp.graphics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (805 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 320 Truth = sci.med Predicted = sci.med\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 321 Truth = sci.med Predicted = sci.med\n",
      "item 322 Truth = comp.graphics Predicted = comp.graphics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 323 Truth = comp.graphics Predicted = comp.graphics\n",
      "item 324 Truth = soc.religion.christian Predicted = soc.religion.christian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1888 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 325 Truth = comp.graphics Predicted = comp.graphics\n",
      "item 326 Truth = sci.med Predicted = sci.med\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1149 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 327 Truth = soc.religion.christian Predicted = soc.religion.christian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 328 Truth = sci.med Predicted = soc.religion.christian\n",
      "item 329 Truth = soc.religion.christian Predicted = soc.religion.christian\n",
      "item 330 Truth = comp.graphics Predicted = comp.graphics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2945 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 331 Truth = sci.med Predicted = sci.med\n",
      "item 332 Truth = comp.graphics Predicted = comp.graphics\n",
      "item 333 Truth = comp.graphics Predicted = comp.graphics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 334 Truth = sci.med Predicted = comp.graphics\n",
      "item 335 Truth = comp.graphics Predicted = comp.graphics\n",
      "item 336 Truth = soc.religion.christian Predicted = soc.religion.christian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2080 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 337 Truth = soc.religion.christian Predicted = soc.religion.christian\n",
      "item 338 Truth = soc.religion.christian Predicted = soc.religion.christian\n",
      "item 339 Truth = sci.med Predicted = sci.med\n",
      "item 340 Truth = comp.graphics Predicted = comp.graphics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 341 Truth = comp.graphics Predicted = comp.graphics\n",
      "item 342 Truth = soc.religion.christian Predicted = soc.religion.christian\n",
      "item 343 Truth = soc.religion.christian Predicted = soc.religion.christian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 344 Truth = soc.religion.christian Predicted = soc.religion.christian\n",
      "item 345 Truth = sci.med Predicted = sci.med\n",
      "item 346 Truth = sci.med Predicted = sci.med\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1742 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 347 Truth = comp.graphics Predicted = comp.graphics\n",
      "item 348 Truth = soc.religion.christian Predicted = soc.religion.christian\n",
      "item 349 Truth = comp.graphics Predicted = comp.graphics\n",
      "accuracy = 0.94\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "start_idx = 300\n",
    "end_idx = 350\n",
    "for j in range(start_idx, end_idx):\n",
    "    truth = twenty_train.target_names[twenty_train.target[j]]\n",
    "    predicted = predict_news_article(twenty_train.data[j], bert_model, news_classifier)\n",
    "    print(\"item \" + str(j) + \" Truth = \" + truth + \" Predicted = \" + predicted)\n",
    "\n",
    "    if truth == predicted: \n",
    "        correct += 1\n",
    "    \n",
    "print(\"accuracy = \" + str(correct/(end_idx - start_idx))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Religious Article Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We started with Genesis 1 and God's intentions for his world. The story concluded, 'Then God said, 'Let us make people in our image. He made a man out of the dust of the earth and God breathed his spirit into the man. So Adam became a living being. Later God put Adam to sleep and took one of his ribs and made a wife, Eve, for him. God said, 'Rule over the animals … multiply and fill the earth.' Finally God looked at everything he had made and blessed it. He said, 'It is very good.' On the seventh day God rested from his work because he had completed the work of creation.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soc.religion.christian'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_news_article(\"We started with Genesis 1 and God's intentions for his world. The story concluded, 'Then God said, 'Let us make people in our image. He made a man out of the dust of the earth and God breathed his spirit into the man. So Adam became a living being. Later God put Adam to sleep and took one of his ribs and made a wife, Eve, for him. God said, 'Rule over the animals … multiply and fill the earth.' Finally God looked at everything he had made and blessed it. He said, 'It is very good.' On the seventh day God rested from his work because he had completed the work of creation.'\", bert_model=bert_model, classifier=news_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Science Article Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regulators in the United States have already approved deep brain stimulation for the treatment of Parkinson's disease, epilepsy, essential tremor, and obsessive-compulsive disorder. The treatment involves implanting wires into the brain and a stimulator in the chest or abdomen. The stimulator sends small electrical pulses to the wires along a connection lead under the skin. Doctors sometimes refer to the stimulator as a pacemaker. The surgeons implant the wires into areas of the brain that are responsible for the symptoms of the particular condition. In the case of Parkinson's disease, for example, they implant them into the brain area that controls movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sci.med'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_news_article(\"Regulators in the United States have already approved deep brain stimulation for the treatment of Parkinson's disease, epilepsy, essential tremor, and obsessive-compulsive disorder. The treatment involves implanting wires into the brain and a stimulator in the chest or abdomen. The stimulator sends small electrical pulses to the wires along a connection lead under the skin. Doctors sometimes refer to the stimulator as a pacemaker. The surgeons implant the wires into areas of the brain that are responsible for the symptoms of the particular condition. In the case of Parkinson's disease, for example, they implant them into the brain area that controls movement.\", bert_model=bert_model, classifier=news_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computer Graphics Article Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper presents a surface reconstruction algorithm that takes an unoriented point cloud as input and produces an interpolating surface in the form of triangulation. Based on region-growing and Delaunay approaches, this algorithm aims to address the difficulties of reconstruction from point data with imperfections. Starting with a seed triangle from the Delaunay tetrahedron result for input points, the surface is gradually formed by adding the linked Delaunay triangle from the current boundaries one by one. During surface growth, the topology errors and the quantity of the holes generated by adding inappropriate triangles can be reduced by changing the triangle selection criteria and adjusting the addition order of the triangles. We evaluated our method using a wide range of datasets, and this method compares well to popular classic and current algorithms with unoriented input points and triangulated surface output. In addition, to achieve results with a small number of holes on the generated surface, a detection and repair approach is proposed to turn the holes of various shapes into smooth surfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.graphics'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_news_article(\"This paper presents a surface reconstruction algorithm that takes an unoriented point cloud as input and produces an interpolating surface in the form of triangulation. Based on region-growing and Delaunay approaches, this algorithm aims to address the difficulties of reconstruction from point data with imperfections. Starting with a seed triangle from the Delaunay tetrahedron result for input points, the surface is gradually formed by adding the linked Delaunay triangle from the current boundaries one by one. During surface growth, the topology errors and the quantity of the holes generated by adding inappropriate triangles can be reduced by changing the triangle selection criteria and adjusting the addition order of the triangles. We evaluated our method using a wide range of datasets, and this method compares well to popular classic and current algorithms with unoriented input points and triangulated surface output. In addition, to achieve results with a small number of holes on the generated surface, a detection and repair approach is proposed to turn the holes of various shapes into smooth surfaces.\", bert_model=bert_model, classifier=news_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
